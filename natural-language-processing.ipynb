{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "This notebook contains my notes from the *Natural Language Processing* talk by Brian Sletten at *UberConf 2019*. I've done ML on the vision side, but don't know much about NLP or even linguistics in general. I hoped this talk would explain some of the basic principles in NLP.\n",
    "\n",
    "### Introduction\n",
    "Some common NLP tasks:\n",
    "* Search and retrieval\n",
    "* Entity and relashionship extraction\n",
    "* Linguistic structure\n",
    "* Translation\n",
    "* Generative content\n",
    "* Question answering\n",
    "\n",
    "Brian gave a list of common NLP frameworks:\n",
    "* GATE\n",
    "* LingPipe\n",
    "* Apache OpenNLP\n",
    "* UIMA\n",
    "* Stanford Parser\n",
    "* Mallet\n",
    "\n",
    "The above libraries are older and use non-ML techniques.\n",
    "\n",
    "From here, Brian transitioned into talking about several NLP models.\n",
    "\n",
    "### Vector Space Model\n",
    "This model is an old model used for search and retrieval. I have seen parts of this applied to vision algorithms as well. It involves coming up with a *vector representation* of documents, then applying a *similarity measure* to the vectors to decide how well they match up.\n",
    "\n",
    "This can be used nicely in search where you have a query vector trying to find the best matches in a collection of documents.\n",
    "\n",
    "#### Bag of words model\n",
    "This model creates a single vector that tracks of occurrences of words in the document. The following are some variations you can use:\n",
    "* 0/1 present/absent\n",
    "* Word count in the document\n",
    "* Term frequency (TF)\n",
    "* Inverse document frequency (IDF)\n",
    "* TF-IDF combination\n",
    "\n",
    "This approach throws away information, namely the order of occurrences of words.\n",
    "\n",
    "#### N-gram model\n",
    "Like a bag of words, but looking at sequences of words (\"john likes\", \"mary talks\", etc.) as the dictionary instead of individual words.\n",
    "\n",
    "#### Similarity Measures\n",
    "Once you have the vectors defined, you can use a similarity measure to compare the vectors, such as the following:\n",
    "* Dot product\n",
    "* Cosine similarity\n",
    "\n",
    "A downside of the vector space model is that it treats words as isolated indices. It doesn't understand ontologies like \"a cat is an animal, so searching for animal should potentially return results with cats in them\".\n",
    "\n",
    "### Word Embeddings\n",
    "We often Want to be able to group words that have similar meanings. Brian showed an example of the Word2Vec model. He went through a bit of information about how the Word2Vec RNN was trained. \n",
    "\n",
    "He also went through parts of the following tutorials:\n",
    "\n",
    "* http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "* https://deeplearning4j.org/docs/latest/deeplearning4j-nlp-word2vec\n",
    "\n",
    "#### Visualizing Embeddings\n",
    "Brian showed a visualization of the Word2Vec high-dimensional word embeddings being visualized using t-SNE. He talked about the relationships between words that you can see in Word2Vec using math operations on the vectors (King - Man + Woman = Queen).\n",
    "\n",
    "### Naive Bayes\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' Theorem. An advantage of this when used with NLP is that it doesn't require much training data to be effective. This is often used for document classification. The \"naive\" part comes because it assumes independence of the features.\n",
    "\n",
    "Brian went through an example of training a spam classifier in R using Naive Bayes.\n",
    "\n",
    "### Takeaways\n",
    "* He used a lot of linguistics terms I didn't understand, learn about that a bit.\n",
    "* Go through some of those tutorials myself\n",
    "* Try out some of those open-source (non-ML) frameworks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
